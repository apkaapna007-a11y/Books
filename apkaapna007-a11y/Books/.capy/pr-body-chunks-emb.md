This PR adds an embeddings-ready dataset and setup SQL:

- uploads/chunks.csv — 3,290 rows of 1200-token chunks with 200-token overlap generated from the 26 chapter sources. Columns match `public.nelson_book_contents` (excluding id/timestamps). One row per chunk with `chunk_no` per document.
- sql/pgvector_setup.sql — enables pgvector and moddatetime, ensures table exists, adds `embedding vector(1536)`, and creates the `handle_updated_at` trigger if missing.

Notes
- `meta` includes the source filename and chunking parameters.
- `summary` is a short extract from the start of each chunk.

Next steps
- Import `chunks.csv` into `public.nelson_book_contents`.
- Compute embeddings (1536-d) and update the `embedding` column.


₍ᐢ•(ܫ)•ᐢ₎ Generated by [Capy](https://capy.ai) ([view task](https://capy.ai/project/6575c76f-ba70-41c6-955f-1f3b723a9a54/task/3e182ba6-df4d-499d-995d-c3dc54a521e7))